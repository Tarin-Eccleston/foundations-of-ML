{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as sk\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from scipy.stats import ttest_rel\n",
    "\n",
    "import graphviz\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "cv_const = 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: Load the datasets and deal with missing values if applicable in a proper way and describe how you did it. One way you can do it is to replace the value with the mean value of the feature in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets\n",
    "phishing_df = pd.read_csv(r'../data/website-phishing.csv')\n",
    "bcp_df = pd.read_csv(r'../data/BCP.csv')\n",
    "arrhythmia_df = pd.read_csv(r'../data/arrhythmia.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean data, missing values in particular\n",
    "\n",
    "# convert \"?\" to NaN for readibility\n",
    "phishing_df.replace('?', np.nan, inplace=True)\n",
    "bcp_df.replace('?', np.nan, inplace=True)\n",
    "arrhythmia_df.replace('?', np.nan, inplace=True)\n",
    "\n",
    "# int for categorical \n",
    "# float for continuous\n",
    "\n",
    "phishing_df = phishing_df.astype(int)\n",
    "# unsure which features are numeric/categorical so make them all categorical\n",
    "bcp_df = bcp_df.astype(int)\n",
    "arrhythmia_df = arrhythmia_df.astype(float)\n",
    "# only sex and class are categorical\n",
    "arrhythmia_df[\" sex\"] = arrhythmia_df[\" sex\"].astype(int)\n",
    "arrhythmia_df[\"class\"] = arrhythmia_df[\"class\"].astype(int)\n",
    "\n",
    "# convert NaN values in each column to the mean of data in that column if it's continuous\n",
    "# and the mode if it's categorical\n",
    "# remove columns where 50%+ of the entries are missing values\n",
    "\n",
    "dfs = [phishing_df, bcp_df, arrhythmia_df]\n",
    "\n",
    "for df in dfs:\n",
    "    for col in df.columns:\n",
    "        if df[col].isna().sum() / len(df) > 0.5:\n",
    "            df.drop(col, axis=1, inplace=True)\n",
    "        elif df[col].isna().sum() > 0:\n",
    "            if df[col].dtype == int:\n",
    "                # \n",
    "                df[col].fillna(df[col].mode(), inplace=True)\n",
    "            else:\n",
    "                df[col].fillna(df[col].mean(), inplace=True)\n",
    "\n",
    "# rename class column for consistency when using for loops\n",
    "phishing_df.rename(columns={phishing_df.columns[-1]: 'class'}, inplace=True)\n",
    "bcp_df.rename(columns={bcp_df.columns[-1]: 'class'}, inplace=True)\n",
    "arrhythmia_df.rename(columns={arrhythmia_df.columns[-1]: 'class'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, replace all question marks with NaN values for readability. There are many missing values, especially in the arrhythmia dataset. Two methods can be used to replace missing values. The choice between using the mode or mean for imputing missing values depends on the type of variable and the distribution of values. The mode is appropriate for categorical variables, while the mean is appropriate for normally distributed continuous variables. In this example, I looked through each column in the three dataframes independantly, and then decided which columns were most likely categorical or continuous. I would set the data type as int for categorical, and float for continuous. \n",
    "\n",
    "I loop through each column in each dataframe, and filled in missing values denoted by NaN with the mean or mode value of the column depending on if the column data is numeric or categorical. Columns with more than 50% of data as missing values are dropped as there isn't enough data to infer an accurate mean or mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: Implement (1) a decision stump, (2) an unpruned decision tree, (3) a pruned decision tree. Apply (1)-(3) on each dataset. You can use scikit-learn packages. You can use pre-pruning and / or post-pruning techniques as your pruning strategy to obtain the pruned decision tree. Explain the pruning techniques you used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: [0.8809164908049443, 0.9219512195121952, 0.5735294117647058]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tarineccleston/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_split.py:676: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# decision stump\n",
    "# max depth is = 1\n",
    "\n",
    "# collect cross-validation scores for task 4\n",
    "cv_scores_stump = []\n",
    "\n",
    "# store accuracies for each dataset\n",
    "stump_accuracies = []\n",
    "\n",
    "for df in dfs:\n",
    "    X = df.drop([\"class\"], axis=1)\n",
    "    y = df[\"class\"]\n",
    "\n",
    "    # split dataset into training and testing sets\n",
    "    # hyperparameters: training and testing split percentage\n",
    "    # random state for reproducibility\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "    # create a decision stump model\n",
    "    model = DecisionTreeClassifier(max_depth=1, criterion=\"entropy\")\n",
    "\n",
    "    # fit the model to the training data\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # make predictions on the testing data\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # calculate the accuracy of the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    stump_accuracies.append(accuracy)\n",
    "    \n",
    "    # get cross-validation scores\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=cv_const)\n",
    "    cv_scores_stump.append(cv_scores)\n",
    "    \n",
    "    '''\n",
    "    # find a way to print this out as multiple graphs\n",
    "    graph = graphviz.Source(tree.export_graphviz(model, out_file=None))\n",
    "    graph.render(\"model\")\n",
    "    print(tree.plot_tree(model))\n",
    "    '''\n",
    "    \n",
    "print(\"Accuracy:\", stump_accuracies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tarineccleston/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_split.py:676: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: [0.9553813687066627, 0.9463414634146341, 0.625]\n"
     ]
    }
   ],
   "source": [
    "# unpruned decision tree\n",
    "# don't specify max depth\n",
    "# the tree stops growing as the impurity of the split (measured by entropy)\n",
    "# does not decrease by a certain threshold\n",
    "\n",
    "# collect cross-validation scores for task 4\n",
    "cv_scores_unpruned_tree = []\n",
    "\n",
    "# store accuracies for each dataset\n",
    "unpruned_tree_accuracies = []\n",
    "\n",
    "for df in dfs:\n",
    "    X = df.drop([\"class\"], axis=1)\n",
    "    y = df[\"class\"]\n",
    "\n",
    "    # split dataset into training and testing sets\n",
    "    # hyperparameters: training and testing split percentage\n",
    "    # random state for reproducibility\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "    # create a decision tree model with no stopping conditions\n",
    "    model = DecisionTreeClassifier(criterion=\"entropy\")\n",
    "\n",
    "    # fit the model to the training data\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # make predictions on the testing data\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # calculate the accuracy of the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    unpruned_tree_accuracies.append(accuracy)\n",
    "    \n",
    "    # get cross-validation scores\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=cv_const)\n",
    "    cv_scores_unpruned_tree.append(cv_scores)\n",
    "    \n",
    "    '''\n",
    "    # find a way to print this out as multiple graphs\n",
    "    graph = graphviz.Source(tree.export_graphviz(model, out_file=None))\n",
    "    graph.render(\"model\")\n",
    "    print(tree.plot_tree(model))\n",
    "    '''\n",
    "    \n",
    "print(\"Accuracy:\", unpruned_tree_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: [0.9351823937292735, 0.9463414634146341, 0.6985294117647058]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tarineccleston/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_split.py:676: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# pre-pruned decision tree\n",
    "# manually tweak the max depth (stopping condition), where setting the max depth is a pre-pruning technique\n",
    "# find the max depth which allows for the \"best\" accuracy for all datasets\n",
    "\n",
    "# collect cross-validation scores for task 4\n",
    "cv_scores_pruned_tree = []\n",
    "\n",
    "# store accuracies for each dataset\n",
    "pruned_tree_accuracies = []\n",
    "\n",
    "for df in dfs:\n",
    "    X = df.drop([\"class\"], axis=1)\n",
    "    y = df[\"class\"]\n",
    "\n",
    "    # split dataset into training and testing sets\n",
    "    # hyperparameters: training and testing split percentage\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "    # create a decision tree model\n",
    "    model = DecisionTreeClassifier(max_depth = 7)\n",
    "\n",
    "    # fit the model to the training data\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # make predictions on the testing data\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # calculate the accuracy of the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    pruned_tree_accuracies.append(accuracy)\n",
    "    \n",
    "    # get cross-validation scores\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=cv_const)\n",
    "    cv_scores_pruned_tree.append(cv_scores)\n",
    "\n",
    "    '''\n",
    "    # find a way to print this out as multiple graphs\n",
    "    graph = graphviz.Source(tree.export_graphviz(model, out_file=None))\n",
    "    graph.render(\"model\")\n",
    "    print(tree.plot_tree(model))\n",
    "    '''\n",
    "    \n",
    "print(\"Accuracy:\", pruned_tree_accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-pruning techniques involve setting constraints on the growth of the decision tree before the tree is built. One common pre-pruning technique is to set the maximum depth of the decision tree, which limits the number of splits that can be made from the root node to any leaf node. This can be seen as an input max_depth, to the function which creates our classifier (DecisionTreeClassifier). In this example, the unpruned decision tree doesn't specify a max depth, and will continue to grow until the impurity of the split (measured by entropy) of a particular branch does not decrease by a certain threshold. In the pruned decision tree, I manually trialed several different max_depths until I found that a max_depth of 7 was ideal and provided a good balance of complexity and accuracy.\n",
    "\n",
    "By limiting the depth of the tree, pre-pruning can prevent the model from fitting the training data too closely and becoming overfit. This can lead to better generalization performance on new, unseen data. The accuracy on the unpruned and pre-pruned decision trees are roughly similar for the phishing and BCP datasets, however the pre-pruned tree has a greater accuracy than the unpruned tree for the arrythmia dataset. This is because the unpruned decision tree is overfit to the dataset, and doesn't have as good performance on new unseen test data.\n",
    "\n",
    "Limiting the depth of the tree by pre-pruning can reduce the number of splits that need to be evaluated during the training process, which can speed up the training time. The maximum depth hyperparameter must be tuned to the specific dataset, which can be time-consuming and requires expertise. Which leads us onto the next question..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: Use a proper way to select your hyperparameters. Explain how you did it. Explain the observation you got from different datasets, and discuss the possible reason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-pruned Decision Tree Best Params: {'max_depth': 30, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "Accuracy of Pre-pruned Decision Tree:  0.9596020500452216\n",
      "Pre-pruned Decision Tree Best Params: {'max_depth': 6, 'min_samples_leaf': 4, 'min_samples_split': 10}\n",
      "Accuracy of Pre-pruned Decision Tree:  0.9512195121951219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tarineccleston/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_split.py:676: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-pruned Decision Tree Best Params: {'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 10}\n",
      "Accuracy of Pre-pruned Decision Tree:  0.6691176470588235\n"
     ]
    }
   ],
   "source": [
    "# pre-pruned decision tree\n",
    "# automatically find the best hyperparameters using GridSearchCV\n",
    "# the best hyperparameters \n",
    "\n",
    "# store accuracies for each dataset\n",
    "opt_pruned_tree_accuracies = []\n",
    "\n",
    "best_params = []\n",
    "\n",
    "# hyperparameters to fine-tune\n",
    "dt_params = {\n",
    "    'max_depth': [2, 4, 6, 8, 10, 20, 30, None], # maximum depth of the tree, can help prevent overfitting\n",
    "    'min_samples_split': [2, 5, 10], # the minimum number of samples required to split a node\n",
    "    'min_samples_leaf': [1, 2, 4] # the minimum number of samples required to be at a leaf node.\n",
    "}\n",
    "\n",
    "\n",
    "for df in dfs:\n",
    "    X = df.drop([\"class\"], axis=1)\n",
    "    y = df[\"class\"]\n",
    "\n",
    "    # split dataset into training and testing sets\n",
    "    # hyperparameters: training and testing split percentage\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "    # create a decision tree model\n",
    "    model = DecisionTreeClassifier()\n",
    "\n",
    "    # GridSearchCV for each classifier\n",
    "    dt_grid = GridSearchCV(DecisionTreeClassifier(random_state=1), dt_params, cv=cv_const)\n",
    "\n",
    "    # Train the fine-tuned models\n",
    "    dt_grid.fit(X_train, y_train)\n",
    "\n",
    "    # get the best models \n",
    "    dt_best = dt_grid.best_estimator_\n",
    "    print(\"Pre-pruned Decision Tree Best Params: \" + str(dt_grid.best_params_))\n",
    "    \n",
    "    # evaluate the model\n",
    "    # make predictions on the testing data\n",
    "    y_pred = dt_best.predict(X_test)\n",
    "\n",
    "    # calculate the accuracy of the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"Accuracy of Pre-pruned Decision Tree: \", accuracy)\n",
    "\n",
    "    '''\n",
    "    # find a way to print this out as multiple graphs\n",
    "    graph = graphviz.Source(tree.export_graphviz(model, out_file=None))\n",
    "    graph.render(\"model\")\n",
    "    print(tree.plot_tree(model))\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are only concerned with prepruned decision trees for this example.\n",
    "\n",
    "Hyperparameter tuning is the process of finding the best hyperparameters for a machine learning model. Grid search is a common brute-force method for hyperparameter tuning where a predefined hyperparameter space is exhaustively searched. The GridSearchCV function in Python's scikit-learn library can be used to perform grid search. The resulting object's best_estimator_ attribute can be utilized to access the model with the best set of hyperparameters discovered during the search.\n",
    "\n",
    "Firstly, choose common hyperparameters in decision trees such as max_depth, min_samples_split and min_samples_leaf.\n",
    "- Max_depth: the maximum depth a decision tree can reach\n",
    "- Min_samples_split: a hyperparameter in decision tree algorithms that controls the minimum number of samples required to split an internal node. \n",
    "- Min_samples_leaf: a hyperparameter for decision tree algorithms in scikit-learn that specifies the minimum number of samples required to be at a leaf node.\n",
    "\n",
    "Create a test grid using a range of values for each hyperparameter. Split the data into training and testing. Then create the decision tree classifier with no default hyperparameters, then create a search grid for the bare decision tree model using different combinations of hyperparameters. Then train each model in the search grid using the training set using 10-fold cross-validation.\n",
    "\n",
    "Cross-validation is a statistical technique used to evaluate the performance of a machine learning model. In this example, we divide our training dataset into 10 subsets, train the model on one subset or fold, and then evaluate it on the remaining 9 folds. This process is repeated for each fold, and the average performance across all folds is used as the final evaluation metric. The primary goal of cross-validation is to assess how well the model will generalize to new, unseen data. By evaluating the model on multiple subsets of the data, it helps to reduce the risk of overfitting, which occurs when the model performs well on the training data but poorly on new data\n",
    "\n",
    "Once all our models in the grid search have been evaluated, we can then find the one which performs the best overall in accuracy and F1-score. From this model, we can finally extract the most optimal hyperparameter values and use that to build our new model.\n",
    "\n",
    "Observations\n",
    "- Phishing: The max depth of the \n",
    "- BCP:\n",
    "- Arrythmia: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4: Compare the three methods used in task 2 and determine if any are performing significantly worse on each dataset. Report the p-value for the significance tests. Explain why the worst method performs worse than others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use cross validation scores from Question 2\n",
    "\n",
    "# \n",
    "for df in dfs:\n",
    "    p_value_stump_unpruned = ttest_rel(cv_scores_stump, cv_scores_unpruned_tree)[1]\n",
    "    p_value_stump_prepruned = ttest_rel(cv_scores_stump, cv_scores_prepruned_tree)[1]\n",
    "    p_value_pruned_prepruned = ttest_rel(cv_scores_unpruned_tree, cv_scores_prepruned_tree)[1]\n",
    "\n",
    "# Create the table data\n",
    "table_data = [    ['Decision Tree vs Random Forrest', p_value_dt_rf],\n",
    "    ['Decision Tree vs Bagging', p_value_dt_bagging],\n",
    "    ['Random Forrest vs Bagging', p_value_rf_bagging]\n",
    "]\n",
    "\n",
    "# Print the table\n",
    "headers = ['Comparison', 'P-value']\n",
    "print(tabulate(table_data, headers=headers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5: Use a pruning strategy (could be single or a combination of pruning techniques) that is different from what you've used in task 2. For example, if you use pre-pruning in task 2, then you need to use post-pruning or a combining of pre-pruning or post-pruning here. If you use maximum tree depth in task 2, you can't use the minimum data points in each leaf as the only pruning technique to control the tree size in task 5. This is because, in principle, they are similar techniques. Compare the effects of the two different pruning strategies on the three datasets. Explain your findings and discuss the possible reason."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 5: use post pruning method and compare that to task 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
